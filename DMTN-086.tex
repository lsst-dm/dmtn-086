

\documentclass[DM,authoryear,toc]{lsstdoc}
% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html

% Package imports go here.

% Local commands go here.

% To add a short-form title:
% \title[Short title]{Title}
\title{Next-to-the-Database Processing Use Cases}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
Colin Slater
}

\setDocRef{DMTN-086}

\date{\today}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
A set of candidate use cases for a next-to-the-database processing systsem.
}

% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1}{2018-03-25}{Initial draft.}{Colin Slater}
  \addtohist{2}{2018-06-18}{Add Galaxies \& Cosmology.}{Colin Slater}
  \addtohist{2}{2018-07-19}{Add Solar System example.}{Colin Slater}
}

\begin{document}

% Create the title page.
% Table of contents is added automatically with the "toc" class option.
\maketitle

This document describes use cases that may be productively served by a
next-to-the-database processing system. The intention is to span a wide range
of cases in terms of science domain, data volume and CPU usage. Depending on
the actual implementation of the next-to-the-database system, some of these
cases may be better served by execution on a relational database, or on a
batch processing system. This document is meant to be broadly inclusive of
these cases in order to inform decisions on the delineations between these
systems. Thus these use cases are \textit{\underline{not}} requirements, and
should not be used to derive requirements without DM CCB approval.

\section{Milky Way}

\subsection{Milky Way Density Maps}
\label{mw_density}

\begin{itemize}
  \item \textbf{Goal}: Produce all-sky maps of stellar density.
  \item \textbf{Data}: Object table, psRa, psDecl, [gri]PsFlux, [gri]PsFluxSigma,
  extendedness, flags.
  \item \textbf{Processing}:
    \begin{enumerate}
      \item Select a set of stars based on \texttt{extendedness} and color.
      Typical selections are for main-sequence turn-off stars, or blue
      horizontal branch stars, or giants. Apparent magnitude cuts can be used as
      a proxy for distance cuts, creating maps in spherical shells around the
      Sun.
      \item Pass these stars to a function which projects them onto an
      equal-area coordinate system, which is split into ~1000x1000 pixels
      covering the entire sky. Additional preprocessing of stars, such as
      dereddening, may also be applied.
      \item Count stars falling into each pixel; essentially creating a 2D
      histogram. This can be computed in several spatial partitions and then
      directly summed. The output data volume is of order megabytes, but more
      sophisticated versions returning several distance slices simultaneously
      could be hundreds of megabytes.
    \end{enumerate}
  \item \textbf{Performance Expectation}: This process is expected to be
  IO-limited, as the coordinate transform and binning is inexpensive. One could
  reasonably expect similar performance on this as any other full table scan.
\end{itemize}

\subsection{Maps of Stellar Properties}

\begin{itemize}
  \item \textbf{Goal}: Produce maps of mean quantities in spatial cells across
  the Milky Way. This could be mean metallicities, proper motions, or other
  transformed velocities,
  \item \textbf{Data}: Point-source photometry from \texttt{Object}, as in
  case~\ref{mw_density}, Optionally psParallax, psMuRa, PsMuDecl, and their
  associated uncertainties.
  \item \textbf{Processing}:
    \begin{enumerate}
      \item Select the target type of stars, as in case~\ref{mw_density}.
      \item Reproject into the desired output coordinate system. For example,
      many maps of this sort look at a vertical slices through the Galaxy, as in
      Figure 7.2 in the Science Book.
      \item Estimate, for example, the metallicity of each star in the sample
      based on \texttt{ugri} photometry, as per Ivezi\'{c} et al. 2008.
      Alternatively, there are numerous dynamical computations one can perform,
      particularly when accurate proper motions are available.
      \item For each spatial bin compute a summary statistic on the quantity of
      interest. Computing the mean is easily understandable and can be
      parallelized by separately summing the quantity and the number of stars
      included in the summation, then only dividing these two numbers after all
      partitions have been processed.
      \item Return the map, which is of order 1000x1000 pixels. If multiple maps
      are made in different slices through the Galaxy, the data volume scales proportionally.
    \end{enumerate}
    \item \textbf{Performance Expectation}: Similar performance as
    \ref{mw_density}; comparable to other full table scans.
\end{itemize}

\subsection{Finding Milky Way Satellites}
\label{mw_sats}

This case is a variation of Section~\ref{mw_density}, with similar columns but with
a much finer grid of the resulting maps. Dwarf galaxies often appear as
densities in MSTO stars, or in more distant cases, red giant branch stars, and
these overdensities can be much smaller on the sky than the typical halo
substructure studied in case~\ref{mw_density}. The required binning is of order
tens of arcminutes for Milky Way satellites, or single arcminute scales for
satellites of other galaxies. Data volumes are consequently tens to hundreds of
times greater than that of case~\ref{mw_density}.

\subsection{Color-magnitude diagrams}

Given a region on the sky, ~few square degrees, a common method for characterizing
stellar populations is to create a binned color-magnitude diagram. The size
scales for this are larger than the typical ``one point per star''
color-magnitude diagrams, which makes it useful to bin the data while it is
next-to-the-database.

\begin{itemize}
  \item \textbf{Goal}: Create binned CMD of areas ranging from a few arcminutes
  to square degrees.
  \item \textbf{Data}: \texttt{Object} point source photometry in multiple
  colors, \texttt{extendedness}.
  \item \textbf{Process}:
    \begin{enumerate}
      \item Select stars inside a desired region on the sky. This could be
      rectangular RA, Dec bounds, or a circle around a point, etc.
      \item Bin the stars into a 2D-histogram with color as one axis, apparent
      magnitude as the other axis.
      \item Sum all histograms, if computed in multiple partitions.
    \end{enumerate}
  \item \textbf{Performance Expectation}: Ideally, these could be produced on an
  interactive timescale. The data volumes are not inherently very large.
\end{itemize}

\section{Time Series Analysis}

\subsection{Custom Periodograms}

\subsection{Custom light curve features}
\label{lc_features}

A basic step in identifying transient events is to compute a variety of
non-parametric features which broadly characterize the light curve. This
includes quantities such as the speed and amplitude of an initial brightness
increase, the duration of an event, time of peak brightness, or the rate of
dimming after peak brightness. These quantities are useful for pre-selecting
candidates of a given type of transient event, which then may be followed up
with more detailed fitting or classification procedures. While many of these
features will already be computed by LSST, it is likely that users will want to
try new features, or their own measurement methods.

These features may also correspond to classification methods used on the online
alert stream, for which it is useful to have an offline method for testing new
classification methods before relying on them in the nightly alert stream processing.

\begin{itemize}
  \item \textbf{Goal}: Identify non-parametric features for light curves, such
  as rate of increase in brightness, duration of an event, or time between first
  measurement and peak brightness. The result could be feature values for a
  large number of Objects, or only those where features exceed some threshold.
  \item \textbf{Data}: \texttt{Object} coordinates and basic magnitudes,
  \texttt{Object\_NonPeriodic} features, \texttt{ForcedSource} or \texttt{DiaForcedSource}.
  \item \textbf{Process:}
    \begin{enumerate}
      \item Pre-select \texttt{Object}s whose periodic or non-periodic features
      make them candidates for the desired feature computation. For example, if
      the goal is to find candidates with a 1.0 magnitude monotonic rise, then a
      "min-to-max variation" feature (if provided by LSST) must be $\ge 1.0$ mag.
      \item For each matching \texttt{Object}, obtain the light curve from
      \texttt{ForcedSource}. Apply a function to each $n$-object window in the
      light curve, in this case computing the change in magnitude between
      visits, per unit time, and returning either a measurement value or a
      boolean for whether a threshold is met.
      \item For each \texttt{Object} meeting a feature threshold (if any),
      return the computed features.
    \end{enumerate}
  \item \textbf{Performance Expectation}: Users can be expected to understand
  that the runtime will scale significantly with the number of Objects that pass
  the initial pre-filtering stage. Simple feature computations, such as for
  monotonically brightening objects, are likely to be IO-bound, though
  arbitrarily complex features could become CPU-bound.
\end{itemize}

\subsection{Light Curve Fitting}

While the basic non-parametric features of a light curve can be used to
determine whether objects are good candidates for particular classes of
transients, more detailed parameters for these candidates are often extracted
with a more detailed light curve fitting process.

\begin{itemize}
  \item \textbf{Goal}: Determine best-fitting light curve parameters, such as
  (using supernovae as an example) amplitude, event duration, and time of peak brightness.
  \item \textbf{Data}: \texttt{Object} coordinates and basic magnitudes,
  \texttt{Object\_NonPeriodic} features or custom user-computed features (as in
  Case~\ref{lc_features}),
  \texttt{ForcedSource} or \texttt{DiaForcedSource}.
  \item \textbf{Process}:
    \begin{enumerate}
      \item For each object passing an initial pre-selection function, pass the
      entire light curve to a fitting function, such as a least-squares fitter.
      \item Return a set of best-fit parameters for each object.
    \end{enumerate}
  \item \textbf{Performance Expectation}: Light curve fitting is very likely to
  be entirely CPU-bound, and users should recognize the need to only pass
  reasonable candidates to the fitting process.
\end{itemize}

\subsection{Light Curve Template Fitting}

An alternative to fitting model light curves is comparing new objects to
pre-existing light curves of known transient type. Computationally, this differs
primarily from light curve fitting by requiring a library of light curves be
available to each fitting process, but is likely to be less CPU-intensive.

\begin{itemize}
  \item \textbf{Goal}: Determine the best-matching light curve from a library of
  templates, along with a temporal shift and possibly scale parameters. \item
  \textbf{Data}: \texttt{Object} coordinates and basic magnitudes,
  \texttt{Object\_NonPeriodic} features or custom user-computed features (as in
  Case~\ref{lc_features}),
  \texttt{ForcedSource} or \texttt{DiaForcedSource}.
  \item \textbf{Process}:
    \begin{enumerate}
      \item For each object passing an initial pre-selection function, pass the
      entire light curve and template library to a template-matching function.
      \item Return a best-fit template and parameters for each object.
    \end{enumerate}
  \item \textbf{Performance Expectation}: Similar caveats apply as in the other
  light curve fitting cases.
\end{itemize}

\subsection{Mini-batch Training of Machine Learning Models}

A slightly more complicated data access pattern is required for machine learning
models where each step in the training process operates on a small subsets of
the data, so-called ``mini-batch'' training. The model parameters (weights) are
updated after each batch, and the batches step through the entire dataset.
Training light curve models is
particularly well-suited to these batch methods, since the entire dataset is far
too large to be processed simultaneously. An example of this method in
\citet{2018NatAs...2..151N} uses a batch size of 500 light curves.

\begin{itemize}
  \item \textbf{Goal}: Train a light curve classification model via mini-batch updates.
  \item \textbf{Data}: A user-provided set of labels (class definitions, e.g. RR
  Lyrae, Cepheids) for \texttt{Object}s, and the corresponding light curves
  (\texttt{ForcedSource}).
  \item \textbf{Process}:
  \begin{enumerate}
    \item Select the number of light curves corresponding to the desired batch
    size, joined with the user-provided class labels.
    \item Pass this batch of records to the machine learning training process,
    resulting in an updated state (e.g. weights in a neural network).
    \item Repeat the process on a new batch, updating the model state until convergence.
  \end{enumerate}
  \item \textbf{Performance Expectation}: For many optimizers, the model update
  is relatively inexpensive compared to the IO required for selecting and
  loading each batch.
\end{itemize}

% Pre-selection of SNe targets may involve crossmatching with neighboring
%g alaxies, and their photo-z

% Custom lightcurve features, specific to something like a type I supernova:
%time of rise, peak brightness, delta M15, change in brightnesss over 15 days
%after peak. Type IIP: Slope of the plateau 100 days after peak brightness.
% Lightcurve fitting, ``fitting'' a parameterized model of a favorite type of object
% Template fitting, have an inventory of known light curves, find the closest match.

% Periodic stars with extra stuff: Subtract off the best fitting period,
%amplitude, characterize the residuals

\section{Galaxies and Cosmology}

\subsection{Galaxy count correlation functions with user-supplied tree code}

The goal of galaxy clustering analyses is to count the number of galaxies,
subselected in some way, that are separated by a given angular distance,
grouped into angular distance bins. These binned counts are then compared to
the counts obtained by counting the separations of random points distributed
uniformly with the same survey geometry as the galaxy sample. 

\begin{itemize}
  \item \textbf{Goal}: Count galaxy separations in bins of angular distance,
  and compare that from to randomly distributed points.
  \item \textbf{Data}: \texttt{Object} table with coordinates and magnitudes
  for galaxies; photometric redshifts from \texttt{Object\_Extra}, survey
  geometry and depth maps (in HEALPix, for example).
  \item \textbf{Process}: 
    \begin{enumerate}
      \item Select galaxies with the desired colors, magnitudes, and redshifts.
      \item Divide the sample into roughly ~$50 \, \textrm{deg}^2$ regions of
      sky, which will be processed independently
      \citep[c.f.][]{2017arXiv170801538T}. Larger regions can be used
      if sufficient memory is available, but smaller regions would introduce
      unacceptable artifacts at the largest correlation scales.
      \item For each spatial region, run a community-standard correlation
      function code (e.g. \texttt{TreeCorr}, which was used by DES,
      \citealt{2017arXiv170801536E}), which
      compute the separations using a KD-tree internally.
      \item For each spatial region, generate a random distribution of points
      similar to a ``mock'' catalog, using the completeness/depth map
      information or other survey characterization data. The correlation
      function code is run on this set of random points, and also on the
      correlation between random points and true galaxies. The random catalog process is repeated 10-100 times.
      \item Either sum the resulting galaxy and random data separately, or
      return all spatial subset results separately to the user for analysis.
    \end{enumerate}
  \item \textbf{Performance Expectation}: This is expected to be an expensive
  process both in CPU and in data IO, particularly if the sample selection is
  done in the same analysis run as the correlation function computation. If
  it were divided into two stages, the sample selection may be IO-bound, and a
  much smaller set of galaxy with only their coordinates could be passed to
  the CPU-bound correlation function code. This task may be overall better
  suited to a batch system, depending on various implementation details.
\end{itemize}


\subsection{Photometric redshift evaluation}

After a model for photometric redshifts has been trained on labeled galaxy
data (obtained via spectroscopy, clustering, or other methods), the model
needs to be applied to a much larger set of galaxies without training data,
possibly including the entire galaxy catalog. The computation for each galaxy
is expected to be relatively simple, but models may be highly varied and
are entirely dependent on the user's chosen methodology. This method is an
alternative to the pre-computed photometric redshifts provided in
\texttt{Object\_Extra}, for specialists who want more control over the
methods.

The model-based (e.g. decision trees or neural networks) case is similar in
data flow to template-based fitting, although the sizes of ancillary data may
differ slightly. Note that there is not a corresponding use case for
\textit{training} the model, since the training set is likely to be several
orders of magnitude smaller than the full LSST \texttt{Object} catalog due to
the need for spectroscopic redshifts. It is expected that training can be performed either via batch computing, notebooks, or user hardware.

\begin{itemize}
  \item \textbf{Goal}: Compute a photometric redshift point estimate, or likelihood function, for all galaxies in the survey.
  \item \textbf{Data}: \texttt{Object} galaxy photometry (e.g. either
  bulge-disk decompositon, or aperture magnitudes, etc.), and possibly shape
  parameters. Photo-z model representation, unlikely to exceed $\sim$MB.
  \item \textbf{Process}: 
  \begin{enumerate}
    \item For each galaxy, pass the \texttt{Object} table parameters to a user-defined function which evaluates the photo-z model, and returns one or more values.
    \item The results should then be stored in a table suitable for joining with the \texttt{Object} table.
  \end{enumerate}
  \item \textbf{Performance Expectation}: Model evaluation is likely to be
  fast, and the data volumes are moderate (because of the need for parameters
  in all ugrizy bands.)
\end{itemize}

\section{Solar System}

\subsection{Asteroid properties}

Because Solar System objects are continuously moving, their observed fluxes
are dependent both on intrinsic object properties and the viewing geometry
between the object, the Earth, and the Sun. Extracting the intrinsic
properties such as albedo, color, or spin rate thus requires removing these
geometric effects by computing the object's position, and thus heliocentric
distance and phase angle, based on its orbit, then fitting an appropriate
phase function to the lightcurve \citep{2010Icar..209..542M}.

The LSST Alert Production Pipeline will perform this fitting for a chosen
model (currently specified as a three-parameter H, G1, G2 model in the DPDD),
but users may want to substitute their own choices. Users will also want to
use the phase- and distance-corrected light curves for more sophsticated
analyses, such as period-fiding or detection of outbursts and cometary activity.

\begin{itemize}
  \item \textbf{Goal}: Compute corrected light curves for Solar System objects.
  \item \textbf{Data}: \texttt{SSObject}s and their corresponding
  \texttt{DIASource}s and orbits.
  \item \textbf{Process}:
  \begin{enumerate}
    \item For each \texttt{SSObject}, use the orbit to compute the position
    of the object at the time of each observation. Based on this, the
    helicoentric and geocentric distances can be computed, along with the
    phase angle. This can be used to fit the phase function slope constants
    (e.g. G1 and G2). The absolute magnitude for each object
    apparition can then be computed.
    \item The mean properties of the object (H, G1, G2) can either be
    reported directly, or variations in the observed fluxes relative to the
    expected fluxes can used for other light curve processing, such as
    period finding or outlier detection.
  \end{enumerate}
  \item \textbf{Performance Expectation}: There are two general timescales on
  which this process may be run. For studies of asteroid families, for
  example, it is useful to have a complete sample produced by Data Release
  Production and analyzed homogeneously. This carries performance
  expectations that are comparable to other large light curve fitting use
  cases, scaled appropriately by the smaller size of the \texttt{SSObject}
  table. Alternatively, users may want to specifically refit newly identified
  objects on a $\sim$ nightly timescale, which would require working on data
  from the Prompt Products database.
\end{itemize}


\section{Internal QA}

This section describes quality assurance tasks that are likely to be performed
by the LSST operations team, but are unlikely to be scientifically useful on
their own. These cases are thus subordinate in importance and may be solved by
custom tools using data products directly from the Science Pipelines code, but
would also fit naturally into a next-to-the-database framework.

\subsection{Measurement Uniformity Across the Camera}

A wide variety of tests of the science pipelines involve verifying that measured
physical quantities are not correlated with observational ``nuisance
parameters''. For example, objects should not appear fainter only when it falls
on a specific CCD. While the pipelines are designed to calibrate out such
effects as best they can, it is often possible to identify uncorrected
deficiencies in these calibrations when large numbers of measurements are
available.

\begin{itemize}
  \item \textbf{Goal}: Identify spatial features in the photometric or
  astrometric calibration of images.
  \item \textbf{Data}: \texttt{Object} point source photometry, \texttt{Source}
  point source photometry for selected visits.
  \item \textbf{Process}:
  \begin{enumerate}
    \item For visits matching some criteria (e.g. good seeing, bad seeing, high
    or low airmass, etc), select a subset of \texttt{Source}s with ``good quality''
    photometry or astrometry (non-variable, no flags, likely non-galaxies).
    \item For each source, compute the difference between its measured magnitude
    and the mean magnitude reported in the \texttt{Object} table, or between the
    \texttt{Source}'s position and the mean on-sky position.
    \item Bin the sources by focal plane coordinates, and compute the mean of
    these residuals.
  \end{enumerate}
  \item \textbf{Performance Expectation}: Computing these maps for very large
  sets of visits requires a very high data volume, and can be expected to be no
  faster than the \texttt{Source} shared scan throughput. Analysis of smaller
  sets of visits, of order dozens to hundreds of images, is likely to be a
  semi-interactive task in which results are expected on a shorter timescale.
\end{itemize}

% Include all the relevant bib files.
% https://lsst-texmf.lsst.io/lsstdoc.html#bibliographies
\bibliography{DMTN-086,lsst,lsst-dm,refs_ads,refs,books}

\end{document}
